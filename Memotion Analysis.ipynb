{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "stemmer=SnowballStemmer('english')\n",
    "lemma=WordNetLemmatizer()\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6595, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memotion_data = pd.read_csv(\"data3.csv\")\n",
    "\n",
    "memotion_data .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data1</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Humor</th>\n",
       "      <th>Offensive_or_not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               data1  Sentiment  Humor  \\\n",
       "0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...          5      2   \n",
       "1  The best of #10 YearChallenge! Completed in le...          5      2   \n",
       "2  Sam Thorne @Strippin ( Follow Follow Saw every...          4      3   \n",
       "3             10 Year Challenge - Sweet Dee Edition           4      1   \n",
       "4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...          3      4   \n",
       "\n",
       "   Offensive_or_not  \n",
       "0                 4  \n",
       "1                 4  \n",
       "2                 4  \n",
       "3                 1  \n",
       "4                 1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here Sentiment Humor and offensive_or_not describe the meme\n",
    "# For Sentiment: \n",
    "#  5 -> very_positive\n",
    "#  4 -> positive\n",
    "#  3 -> neutral\n",
    "#  2 -> negative\n",
    "#  1 -> very negative\n",
    "\n",
    "\n",
    "# For Humor: \n",
    "#  5 -> non motivational\n",
    "#  4 -> very twisted\n",
    "#  3 -> not sarcastic\n",
    "#  2 -> General\n",
    "#  1 -> Twisted meaning\n",
    "\n",
    "\n",
    "# For Offensive_or_not: \n",
    "#  5 -> motivational\n",
    "#  4 -> not offensive\n",
    "#  3 -> slight\n",
    "#  2 -> hateful offensive\n",
    "#  1 -> very offensive\n",
    "\n",
    "memotion_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "memotion_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.dropna of                                                   data1  Sentiment  Humor  \\\n",
       "0     LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...          5      2   \n",
       "1     The best of #10 YearChallenge! Completed in le...          5      2   \n",
       "2     Sam Thorne @Strippin ( Follow Follow Saw every...          4      3   \n",
       "3                10 Year Challenge - Sweet Dee Edition           4      1   \n",
       "4     10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...          3      4   \n",
       "5     1998: \"Don't get in car with strangers\" 2008: ...          2      2   \n",
       "6     10 years challenge is lit af Ãœs & B erg 1945 1...          2      3   \n",
       "7     10 Year Challenge emotional edition boredpanda...          3      1   \n",
       "8     Fornite died in 10 months but Minecraft never ...          4      3   \n",
       "9     FACEBOOK '10 YEAR CHALLENGE': A PLOY OR A SIMP...          4      2   \n",
       "10    PROBABLY THE FIRST MAN TO DO 10 YEAR CHALLENGE...          2      2   \n",
       "11    State Dining Room in the White House: 2009 vs....          5      4   \n",
       "12    I did the Facebook 10 year challenge and it wa...          4      2   \n",
       "13    IFIDOWNLOADA MOVIE IN JAMAICA Memes in 2009 AM...          4      2   \n",
       "14    Anti-vaxx kids when they see someone doing the...          1      3   \n",
       "15    I don't get this 10 year challenge  time is no...          3      2   \n",
       "16    When you wakeup and read the messages you sent...          4      1   \n",
       "17    When you're looking through the latest memes a...          5      1   \n",
       "18    Why do 10 years challenge when you can do 20 y...          4      2   \n",
       "19    NO SPOILERS FOR AVENGERS INFINITY WAR! THANOS ...          4      3   \n",
       "20    REMEMBER THIS KID? THIS IS HIM NOW FEEL OLD YET?           3      2   \n",
       "21    WE HAVE A HULK WE HAVE THE RIGHTS TO ALL OUR C...          4      3   \n",
       "22    IS THERE AN ALL-FEMALE AVENGERS MOVIE IN THE W...          3      2   \n",
       "23    I have an army. We have a Hulk. I've gotta jar...          4      2   \n",
       "24    LANGUAGE! (..REALLY? IG BLERD.VISION ...DID WE...          3      3   \n",
       "25    Sucks that the Thanos snap also claimed Captai...          3      3   \n",
       "26    I wonder why Who's that!? ..they call him Hawk...          3      1   \n",
       "27    EVERY TIME I ALREADY KNOW A VERSE \"I UNDERSTOO...          4      1   \n",
       "28    HEY PETER WHAT'S NEW? WELL I GET TO WORK WITH ...          4      2   \n",
       "29    Wants Spider-Man in next \"Avengers\" movie SONY...          5      1   \n",
       "...                                                 ...        ...    ...   \n",
       "6565                                  YOU LYING LET ME           5      2   \n",
       "6566                  WHY HELLO THER memegenerator.net           5      2   \n",
       "6567  Ability When you can make three generations of...          5      4   \n",
       "6568                       Baby Mr. Bean Funny Picture           2      1   \n",
       "6569               WHICH SEAT CAN I TAKE? NOT THIS ONE           3      1   \n",
       "6570  THAT ONE MOMENT WHEN YOU FIND OUT WHATI MEAN m...          5      2   \n",
       "6571  ONBAMemes AKERS a lakception.cow/4506 Kobe Mr....          5      3   \n",
       "6572  MR BEAN AND DR. HOUSE Double the Epicness We K...          4      4   \n",
       "6573  Mr. Bean Face Me Reinvented by SpikePheonix fo...          3      3   \n",
       "6574  YOUR FACE MemeCenter.com WHEN THERES A SUBSTIT...          2      2   \n",
       "6575             SAYS WILL WENDELL WON'T quickmeme.com           4      2   \n",
       "6576  Mr. Bean: NBA Edition! Credit - Alfred Justin ...          4      2   \n",
       "6577  Mr. Bean taught me one thing in life. Enjoy yo...          3      2   \n",
       "6578  amycalvins These Pics Of Mr. Bean Photoshopped...          5      3   \n",
       "6579  No matter how old I grow o _Dekh Bhai 'I will ...          4      3   \n",
       "6580  IDONTALWAYS PLAYTHE PIANO BUT WHEN IDOIES WITH...          4      3   \n",
       "6581  Thanks for making our childhood awesome Mr. Be...          5      2   \n",
       "6582        The new Mr Bean figure is pretty life like           3      2   \n",
       "6583               THE REAL MR. BEAN The real Mr. Bean           3      2   \n",
       "6584  @DCMARVEL_MEMES TAG A FRIEND SO THEY OPEN THEI...          3      2   \n",
       "6585                        I STILL KNOW NOTHING! 2017           2      2   \n",
       "6586  Create a hashtag for this... \"NO TIME TO EXPLA...          4      2   \n",
       "6587  YOU'RE THE BEST AROUND. NOTHING'S EVER GONNA K...          5      1   \n",
       "6588  WHAT OILS ARE GOOD POR CUTTING THAT POUL SMELL...          3      3   \n",
       "6589  If you're going on-and-on-and-on about your pr...          4      3   \n",
       "6590  Tuesday is Mardi Gras Wednesday is Valentine's...          3      1   \n",
       "6591  MUST WATCH MOVIES OF 2017 ITI Chennai memes MA...          3      1   \n",
       "6592  LESS MORE TALKING PLANNING SODA JUNK FOOD COMP...          4      2   \n",
       "6593  When I have time is a fantasy. no one has time...          5      1   \n",
       "6594  The starting point for every good idea is \"Wha...          4      3   \n",
       "\n",
       "      Offensive_or_not  \n",
       "0                    4  \n",
       "1                    4  \n",
       "2                    4  \n",
       "3                    1  \n",
       "4                    1  \n",
       "5                    3  \n",
       "6                    4  \n",
       "7                    4  \n",
       "8                    3  \n",
       "9                    3  \n",
       "10                   1  \n",
       "11                   1  \n",
       "12                   4  \n",
       "13                   4  \n",
       "14                   4  \n",
       "15                   4  \n",
       "16                   3  \n",
       "17                   3  \n",
       "18                   3  \n",
       "19                   4  \n",
       "20                   4  \n",
       "21                   4  \n",
       "22                   3  \n",
       "23                   3  \n",
       "24                   4  \n",
       "25                   4  \n",
       "26                   1  \n",
       "27                   1  \n",
       "28                   3  \n",
       "29                   1  \n",
       "...                ...  \n",
       "6565                 3  \n",
       "6566                 1  \n",
       "6567                 3  \n",
       "6568                 1  \n",
       "6569                 3  \n",
       "6570                 3  \n",
       "6571                 3  \n",
       "6572                 3  \n",
       "6573                 4  \n",
       "6574                 1  \n",
       "6575                 4  \n",
       "6576                 3  \n",
       "6577                 4  \n",
       "6578                 1  \n",
       "6579                 3  \n",
       "6580                 4  \n",
       "6581                 3  \n",
       "6582                 3  \n",
       "6583                 4  \n",
       "6584                 4  \n",
       "6585                 1  \n",
       "6586                 3  \n",
       "6587                 2  \n",
       "6588                 4  \n",
       "6589                 4  \n",
       "6590                 1  \n",
       "6591                 4  \n",
       "6592                 3  \n",
       "6593                 4  \n",
       "6594                 4  \n",
       "\n",
       "[6595 rows x 4 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memotion_data.dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review_col):\n",
    "    review_corpus=[]\n",
    "    for i in range(0,len(review_col)):\n",
    "        review=str(review_col[i])\n",
    "        review=re.sub('[^a-zA-Z]',' ',review)\n",
    "        review=[lemma.lemmatize(w) for w in word_tokenize(str(review).lower())]\n",
    "        review=' '.join(review)\n",
    "        review_corpus.append(review)\n",
    "    return review_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memotion_data['data1']=preprocess(memotion_data.data1.values)\n",
    "memotion_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6595, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking polarity of sentement so dropping others\n",
    "df_train=memotion_data.drop(columns=['Humor','Offensive_or_not'])\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data1</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               data1  Sentiment\n",
       "0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...          5\n",
       "1  The best of #10 YearChallenge! Completed in le...          5\n",
       "2  Sam Thorne @Strippin ( Follow Follow Saw every...          4\n",
       "3             10 Year Challenge - Sweet Dee Edition           4\n",
       "4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...          3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very_positive sentiments: 958\n",
      "positive sentiments: 2912\n",
      "neutral sentiments: 2073\n",
      "negative sentiments: 439\n",
      "very_negative sentiments: 138\n"
     ]
    }
   ],
   "source": [
    "print (\"very_positive sentiments:\", len(df_train[df_train.Sentiment == 5]))\n",
    "print (\"positive sentiments:\", len(df_train[df_train.Sentiment == 4]))\n",
    "print (\"neutral sentiments:\", len(df_train[df_train.Sentiment == 3]))\n",
    "print (\"negative sentiments:\", len(df_train[df_train.Sentiment == 2]))\n",
    "print (\"very_negative sentiments:\", len(df_train[df_train.Sentiment == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6595, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=memotion_data.drop(columns=['Sentiment','Humor','Offensive_or_not'])\n",
    "df_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6595,) (6595,) (6595, 6)\n"
     ]
    }
   ],
   "source": [
    "train_text=df_train.data1.values\n",
    "test_text=df_test.data1.values\n",
    "target=df_train.Sentiment.values\n",
    "y=to_categorical(target)\n",
    "print(train_text.shape,target.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5276,) (5276, 6)\n",
      "(1319,) (1319, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train_text,X_val_text,y_train,y_val=train_test_split(train_text,y,test_size=0.2,stratify=y,random_state=123)\n",
    "print(X_train_text.shape,y_train.shape)\n",
    "print(X_val_text.shape,y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using glove word emdeddings\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11221, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_words = 100\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "num_classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train_text))\n",
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_val = tokenizer.texts_to_sequences(X_val_text)\n",
    "X_test = tokenizer.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5276, 100) (1319, 100) (6595, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "print(X_train.shape,X_val.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test = tokenizer.texts_to_sequences(X_val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Dropout,Embedding,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1122100   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,239,477\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 1,122,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5276 samples, validate on 1319 samples\n",
      "Epoch 1/3\n",
      "5276/5276 [==============================] - 25s 1ms/step - loss: 1.0025 - acc: 0.5072 - val_loss: 0.8539 -\n",
      "val_acc: 0.6494\n",
      "Epoch 2/3\n",
      "5276/5276 [==============================] - 26s 1ms/step - loss: 0.8024 - acc: 0.5699 - val_loss: 0.8116 -\n",
      "val_acc: 0.6717\n",
      "Epoch 3/3\n",
      "5276/5276 [==============================] - 25s 1ms/step - loss: 0.7365 - acc: 0.6045 - val_loss: 0.8114 -\n",
      "val_acc: 0.6713\n",
      "CPU times: user 1min 16s, sys: 27.2 s, total: 1min 43s\n",
      "Wall time: 51s\n"
     ]
    }
   ],
   "source": [
    "history1=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=epochs, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
